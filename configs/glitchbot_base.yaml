# GlitchBot Base Configuration
model:
  name: "GlitchBotTransformer"
  num_layers: 4
  num_heads: 4
  hidden_dim: 256
  vocab_size: 2048
  max_seq_length: 256
  dropout: 0.1
  
data:
  corpus_path: "./data/cleaned_corpus.txt"
  tokenizer_path: "./tokenizer/"
  batch_size: 8
  max_length: 256
  
training:
  epochs: 10
  learning_rate: 3e-4
  weight_decay: 0.01
  warmup_steps: 1000
  gradient_clip: 1.0
  eval_every: 1  # epochs
  save_every: 1000  # steps
  
optimizer:
  name: "AdamW"
  betas: [0.9, 0.95]
  eps: 1e-8
  
scheduler:
  name: "cosine"
  warmup_steps: 1000
  
paths:
  checkpoints: "./checkpoints/"
  models: "./models/"
  logs: "./logs/"
  samples: "./samples/"
  
device:
  mixed_precision: true
  compile_model: false  # Set to true for PyTorch 2.0+
  
evaluation:
  sample_prompts:
    - "The purpose of GlitchBot is to"
    - "Artificial intelligence can help"
    - "In the future, technology will"
    - "Machine learning enables"
  
logging:
  level: "INFO"
  log_file: "./logs/train.log"
  wandb_project: null  # Set project name to enable W&B logging
