{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd608c1c",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch Fundamentals for AI Assistant Development\n",
    "\n",
    "Welcome to Phase 1 of building your AI assistant from scratch! This notebook covers essential PyTorch concepts needed for implementing transformer architectures and language models.\n",
    "\n",
    "## 📚 Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Master PyTorch tensor operations and automatic differentiation\n",
    "- Understand neural network building blocks\n",
    "- Implement attention mechanisms from scratch\n",
    "- Build transformer components\n",
    "- Gain hands-on experience with gradient computation and optimization\n",
    "\n",
    "## 🎯 Prerequisites\n",
    "\n",
    "- Basic Python programming knowledge\n",
    "- Familiarity with linear algebra and calculus\n",
    "- Understanding of machine learning concepts\n",
    "\n",
    "Let's start building your AI assistant from the ground up! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7ab1b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need for our PyTorch fundamentals exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cd57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for PyTorch and deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Numerical and visualization libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec6309",
   "metadata": {},
   "source": [
    "## 2. PyTorch Tensor Operations\n",
    "\n",
    "Tensors are the fundamental building blocks of PyTorch. Let's explore tensor creation, manipulation, and operations that are essential for building neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Tensor Creation\n",
    "print(\"=== Tensor Creation ===\")\n",
    "\n",
    "# Create tensors in different ways\n",
    "zeros_tensor = torch.zeros(3, 4)\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "random_tensor = torch.randn(2, 3, 4)  # Normal distribution\n",
    "uniform_tensor = torch.rand(3, 3)     # Uniform [0, 1]\n",
    "\n",
    "print(f\"Zeros tensor shape: {zeros_tensor.shape}\")\n",
    "print(f\"Random tensor:\\n{random_tensor[0]}\")  # Show first matrix\n",
    "\n",
    "# From Python lists/arrays\n",
    "list_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "from_numpy = torch.from_numpy(numpy_array)\n",
    "\n",
    "print(f\"From list: {list_tensor}\")\n",
    "print(f\"From numpy: {from_numpy}\")\n",
    "\n",
    "# 2.2 Tensor Properties and Device Management\n",
    "print(\"\\n=== Tensor Properties ===\")\n",
    "x = torch.randn(3, 4, 5)\n",
    "print(f\"Shape: {x.shape}\")\n",
    "print(f\"Data type: {x.dtype}\")\n",
    "print(f\"Device: {x.device}\")\n",
    "print(f\"Number of elements: {x.numel()}\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x_gpu = x.to(device)\n",
    "print(f\"Tensor on device: {x_gpu.device}\")\n",
    "\n",
    "# 2.3 Essential Tensor Operations\n",
    "print(\"\\n=== Essential Operations ===\")\n",
    "\n",
    "# Matrix operations\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "C = torch.matmul(A, B)  # Matrix multiplication\n",
    "print(f\"Matrix multiplication: {A.shape} @ {B.shape} = {C.shape}\")\n",
    "\n",
    "# Element-wise operations\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(f\"Addition: {x + y}\")\n",
    "print(f\"Multiplication: {x * y}\")\n",
    "print(f\"Power: {torch.pow(x, 2)}\")\n",
    "\n",
    "# Reduction operations\n",
    "data = torch.randn(3, 4)\n",
    "print(f\"Sum: {torch.sum(data)}\")\n",
    "print(f\"Mean: {torch.mean(data)}\")\n",
    "print(f\"Max: {torch.max(data)}\")\n",
    "print(f\"Sum along axis 1: {torch.sum(data, dim=1)}\")\n",
    "\n",
    "# Reshaping and view operations\n",
    "original = torch.randn(2, 3, 4)\n",
    "reshaped = original.view(6, 4)      # Change shape\n",
    "flattened = original.flatten()      # Flatten to 1D\n",
    "print(f\"Original: {original.shape} -> Reshaped: {reshaped.shape} -> Flattened: {flattened.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cb58c",
   "metadata": {},
   "source": [
    "## 3. Automatic Differentiation (Autograd)\n",
    "\n",
    "Automatic differentiation is the heart of PyTorch's neural network training. Let's explore how gradients are computed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Basic Gradient Computation\n",
    "print(\"=== Basic Gradients ===\")\n",
    "\n",
    "# Simple function: y = x^2 + 3x + 1\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**2 + 3*x + 1\n",
    "\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"y = x^2 + 3x + 1 = {y.item()}\")\n",
    "\n",
    "# Compute gradient dy/dx\n",
    "y.backward()\n",
    "print(f\"dy/dx = 2x + 3 = {x.grad.item()}\")\n",
    "print(f\"At x=2: dy/dx = {2*2 + 3} (analytical) vs {x.grad.item()} (autograd)\")\n",
    "\n",
    "# 3.2 Gradient Computation with Vectors\n",
    "print(\"\\n=== Vector Gradients ===\")\n",
    "\n",
    "# Multi-variable function\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.sum(x**2)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = sum(x^2) = {y.item()}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"dy/dx = 2x = {x.grad}\")\n",
    "\n",
    "# 3.3 Gradient Accumulation\n",
    "print(\"\\n=== Gradient Accumulation ===\")\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x**2\n",
    "y1.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Second computation (gradients accumulate!)\n",
    "y2 = x**3\n",
    "y2.backward()\n",
    "print(f\"After second backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Clear gradients\n",
    "x.grad.zero_()\n",
    "print(f\"After zero_(): x.grad = {x.grad}\")\n",
    "\n",
    "# 3.4 Computational Graph Example\n",
    "print(\"\\n=== Computational Graph ===\")\n",
    "\n",
    "# Create a more complex computation\n",
    "a = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "c = a * b      # c = a * b\n",
    "d = c + a      # d = c + a = a * b + a\n",
    "e = d**2       # e = d^2 = (a * b + a)^2\n",
    "\n",
    "print(f\"a = {a.item()}, b = {b.item()}\")\n",
    "print(f\"c = a * b = {c.item()}\")\n",
    "print(f\"d = c + a = {d.item()}\")\n",
    "print(f\"e = d^2 = {e.item()}\")\n",
    "\n",
    "# Backward pass\n",
    "e.backward()\n",
    "\n",
    "print(f\"de/da = {a.grad.item()}\")\n",
    "print(f\"de/db = {b.grad.item()}\")\n",
    "\n",
    "# Manual verification:\n",
    "# e = (a*b + a)^2 = (a*(b+1))^2\n",
    "# de/da = 2*(a*(b+1))*(b+1) = 2*a*(b+1)^2\n",
    "# de/db = 2*(a*(b+1))*a = 2*a^2*(b+1)\n",
    "print(f\"Manual de/da = 2*a*(b+1)^2 = {2*a.item()*(b.item()+1)**2}\")\n",
    "print(f\"Manual de/db = 2*a^2*(b+1) = {2*a.item()**2*(b.item()+1)}\")\n",
    "\n",
    "# 3.5 Higher-order Derivatives\n",
    "print(\"\\n=== Higher-order Derivatives ===\")\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**4\n",
    "\n",
    "# First derivative\n",
    "grad1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"First derivative dy/dx = {grad1.item()}\")\n",
    "\n",
    "# Second derivative\n",
    "grad2 = torch.autograd.grad(grad1, x)[0]\n",
    "print(f\"Second derivative d²y/dx² = {grad2.item()}\")\n",
    "\n",
    "# Manual verification: y = x^4, dy/dx = 4x^3, d²y/dx² = 12x^2\n",
    "print(f\"Manual: dy/dx = 4x^3 = {4 * x.item()**3}\")\n",
    "print(f\"Manual: d²y/dx² = 12x^2 = {12 * x.item()**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5dd937",
   "metadata": {},
   "source": [
    "## 4. Neural Network Building Blocks\n",
    "\n",
    "Now let's build the fundamental components of neural networks using PyTorch's nn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Basic Neural Network Layers\n",
    "print(\"=== Basic Neural Network Layers ===\")\n",
    "\n",
    "# Linear (fully connected) layer\n",
    "input_size, output_size = 10, 5\n",
    "linear_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "print(f\"Linear layer: {input_size} -> {output_size}\")\n",
    "print(f\"Weight shape: {linear_layer.weight.shape}\")\n",
    "print(f\"Bias shape: {linear_layer.bias.shape}\")\n",
    "\n",
    "# Test with input\n",
    "x = torch.randn(3, input_size)  # Batch of 3 samples\n",
    "output = linear_layer(x)\n",
    "print(f\"Input shape: {x.shape} -> Output shape: {output.shape}\")\n",
    "\n",
    "# 4.2 Activation Functions\n",
    "print(\"\\n=== Activation Functions ===\")\n",
    "\n",
    "# Common activation functions\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "# ReLU\n",
    "relu = F.relu(x)\n",
    "leaky_relu = F.leaky_relu(x, negative_slope=0.1)\n",
    "\n",
    "# Sigmoid and Tanh\n",
    "sigmoid = torch.sigmoid(x)\n",
    "tanh = torch.tanh(x)\n",
    "\n",
    "# GELU (used in transformers)\n",
    "gelu = F.gelu(x)\n",
    "\n",
    "# Plot activation functions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "activations = [\n",
    "    (relu, \"ReLU\"),\n",
    "    (leaky_relu, \"Leaky ReLU\"),\n",
    "    (sigmoid, \"Sigmoid\"),\n",
    "    (tanh, \"Tanh\"),\n",
    "    (gelu, \"GELU\"),\n",
    "]\n",
    "\n",
    "for i, (activation, name) in enumerate(activations):\n",
    "    axes[i].plot(x.numpy(), activation.numpy())\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].grid(True)\n",
    "    axes[i].set_xlabel('x')\n",
    "    axes[i].set_ylabel(f'{name}(x)')\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.3 Building a Simple Neural Network\n",
    "print(\"\\n=== Simple Neural Network ===\")\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Create and test the network\n",
    "model = SimpleNN(input_size=784, hidden_size=128, output_size=10)\n",
    "print(f\"Model architecture:\\n{model}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(32, 784)  # Batch of 32 MNIST-like images\n",
    "output = model(dummy_input)\n",
    "print(f\"Input: {dummy_input.shape} -> Output: {output.shape}\")\n",
    "\n",
    "# 4.4 Loss Functions and Optimization\n",
    "print(\"\\n=== Loss Functions and Optimization ===\")\n",
    "\n",
    "# Cross-entropy loss for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy training step\n",
    "model.train()  # Set to training mode\n",
    "\n",
    "# Forward pass\n",
    "logits = model(dummy_input)\n",
    "targets = torch.randint(0, 10, (32,))  # Random class labels\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(logits, targets)\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()  # Clear gradients\n",
    "loss.backward()        # Compute gradients\n",
    "optimizer.step()       # Update parameters\n",
    "\n",
    "print(f\"Loss after one step: {criterion(model(dummy_input), targets).item():.4f}\")\n",
    "\n",
    "# 4.5 Different Loss Functions\n",
    "print(\"\\n=== Different Loss Functions ===\")\n",
    "\n",
    "# Generate some data for demonstration\n",
    "batch_size = 16\n",
    "predictions = torch.randn(batch_size, 10)\n",
    "targets_classification = torch.randint(0, 10, (batch_size,))\n",
    "targets_regression = torch.randn(batch_size, 10)\n",
    "\n",
    "# Classification losses\n",
    "cross_entropy = nn.CrossEntropyLoss()(predictions, targets_classification)\n",
    "print(f\"Cross Entropy Loss: {cross_entropy.item():.4f}\")\n",
    "\n",
    "# Regression losses\n",
    "mse_loss = nn.MSELoss()(predictions, targets_regression)\n",
    "mae_loss = nn.L1Loss()(predictions, targets_regression)\n",
    "huber_loss = nn.HuberLoss()(predictions, targets_regression)\n",
    "\n",
    "print(f\"MSE Loss: {mse_loss.item():.4f}\")\n",
    "print(f\"MAE Loss: {mae_loss.item():.4f}\")\n",
    "print(f\"Huber Loss: {huber_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479a0e6",
   "metadata": {},
   "source": [
    "## 5. Attention Mechanism Implementation\n",
    "\n",
    "The attention mechanism is the heart of transformer models. Let's implement it from scratch to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Simple Attention Mechanism\n",
    "print(\"=== Simple Attention Mechanism ===\")\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simple attention mechanism for understanding the concept.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_v = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: [batch_size, seq_len, hidden_size]\n",
    "            key: [batch_size, seq_len, hidden_size]  \n",
    "            value: [batch_size, seq_len, hidden_size]\n",
    "            mask: Optional attention mask\n",
    "        \"\"\"\n",
    "        # Transform inputs\n",
    "        Q = self.W_q(query)  # [batch_size, seq_len, hidden_size]\n",
    "        K = self.W_k(key)    # [batch_size, seq_len, hidden_size]\n",
    "        V = self.W_v(value)  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.hidden_size)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test simple attention\n",
    "batch_size, seq_len, hidden_size = 2, 8, 64\n",
    "attention = SimpleAttention(hidden_size)\n",
    "\n",
    "# Create sample sequences\n",
    "sequences = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "# Self-attention (query, key, value are the same)\n",
    "output, weights = attention(sequences, sequences, sequences)\n",
    "\n",
    "print(f\"Input shape: {sequences.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "\n",
    "# Visualize attention weights for first sample\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(weights[0].detach().numpy(), cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.title('Attention Weights Visualization')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()\n",
    "\n",
    "# 5.2 Multi-Head Attention\n",
    "print(\"\\n=== Multi-Head Attention ===\")\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention as used in Transformers.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear transformations for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
    "                                   mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute scaled dot-product attention.\"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, d_model = query.size()\n",
    "        \n",
    "        # Linear transformations and reshape for multi-head attention\n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # Add head dimension\n",
    "        \n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads and put through final linear layer\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attention_output)\n",
    "        return output\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model, num_heads = 512, 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Test input\n",
    "batch_size, seq_len = 2, 10\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output = mha(x, x, x)  # Self-attention\n",
    "print(f\"Multi-head attention input: {x.shape}\")\n",
    "print(f\"Multi-head attention output: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "mha_params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"Multi-head attention parameters: {mha_params:,}\")\n",
    "\n",
    "# 5.3 Positional Encoding\n",
    "print(\"\\n=== Positional Encoding ===\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer models.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create div_term for sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Test positional encoding\n",
    "pos_encoding = PositionalEncoding(d_model=128, max_len=100)\n",
    "\n",
    "# Create sample embeddings\n",
    "seq_len, batch_size, d_model = 20, 4, 128\n",
    "embeddings = torch.randn(seq_len, batch_size, d_model)\n",
    "\n",
    "# Apply positional encoding\n",
    "encoded = pos_encoding(embeddings)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Positionally encoded shape: {encoded.shape}\")\n",
    "\n",
    "# Visualize positional encoding\n",
    "plt.figure(figsize=(12, 6))\n",
    "pe_matrix = pos_encoding.pe[:50, 0, :].numpy()  # First 50 positions\n",
    "plt.imshow(pe_matrix.T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Positional Encoding Visualization')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Embedding Dimension')\n",
    "plt.show()\n",
    "\n",
    "print(\"Position encoding provides unique patterns for each position!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d64c83",
   "metadata": {},
   "source": [
    "## 6. Mini Transformer Block\n",
    "\n",
    "Let's combine everything we've learned to build a basic transformer block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc38b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "# 6.2 Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer decoder block.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Transformer block forward pass with residual connections.\"\"\"\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 6.3 Mini Language Model\n",
    "class MiniLanguageModel(nn.Module):\n",
    "    \"\"\"A very simple language model using transformer blocks.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, \n",
    "                 num_layers: int, d_ff: int, max_seq_len: int = 1024, dropout: float = 0.1):\n",
    "        super(MiniLanguageModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "    \n",
    "    def create_causal_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Create causal mask for autoregressive generation.\"\"\"\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "        return mask.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Token embeddings (scaled by sqrt(d_model))\n",
    "        token_embeds = self.token_embedding(input_ids) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        # Transpose for positional encoding (expects [seq_len, batch_size, d_model])\n",
    "        token_embeds = token_embeds.transpose(0, 1)\n",
    "        hidden_states = self.pos_encoding(token_embeds)\n",
    "        # Transpose back to [batch_size, seq_len, d_model]\n",
    "        hidden_states = hidden_states.transpose(0, 1)\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = self.create_causal_mask(seq_len, device)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            hidden_states = transformer_block(hidden_states, causal_mask)\n",
    "        \n",
    "        # Output projection to vocabulary\n",
    "        logits = self.output_projection(hidden_states)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"Count trainable parameters.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Test the mini language model\n",
    "print(\"=== Mini Language Model Test ===\")\n",
    "\n",
    "# Model configuration\n",
    "vocab_size = 1000\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "d_ff = 1024\n",
    "max_seq_len = 512\n",
    "\n",
    "# Create model\n",
    "model = MiniLanguageModel(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len)\n",
    "\n",
    "print(f\"Model created with {model.count_parameters():,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size, seq_len = 2, 32\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_ids)\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Expected shape: [{batch_size}, {seq_len}, {vocab_size}]\")\n",
    "\n",
    "# Test loss computation\n",
    "targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Reshape for loss computation\n",
    "loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n✅ Mini transformer model working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de45fb0",
   "metadata": {},
   "source": [
    "## 7. 🎯 Key Takeaways and Next Steps\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **PyTorch Fundamentals**\n",
    "   - Tensor operations and device management\n",
    "   - Automatic differentiation (autograd)\n",
    "   - Neural network building blocks\n",
    "\n",
    "2. **Attention Mechanisms**\n",
    "   - Simple attention implementation\n",
    "   - Multi-head attention from scratch\n",
    "   - Positional encoding for sequence modeling\n",
    "\n",
    "3. **Transformer Components**\n",
    "   - Feed-forward networks\n",
    "   - Layer normalization and residual connections\n",
    "   - Complete transformer block implementation\n",
    "\n",
    "4. **Mini Language Model**\n",
    "   - End-to-end transformer-based language model\n",
    "   - Causal masking for autoregressive generation\n",
    "   - Parameter counting and model architecture\n",
    "\n",
    "### What's Next:\n",
    "\n",
    "🔬 **Phase 2**: Set up your training environment with GPU optimization and distributed training capabilities.\n",
    "\n",
    "📊 **Phase 3**: Implement large-scale data collection and preprocessing pipelines.\n",
    "\n",
    "🏗️ **Phase 4**: Build a full-scale transformer model with all the optimizations.\n",
    "\n",
    "🚀 **Phase 5**: Train your first small language model and see it generate text!\n",
    "\n",
    "### 💡 Pro Tips:\n",
    "\n",
    "- **Practice**: Try modifying the model architecture (more layers, heads, etc.)\n",
    "- **Experiment**: Change activation functions, dropout rates, layer normalization\n",
    "- **Understand**: Make sure you understand each component before moving forward\n",
    "- **Debug**: Use `print()` statements and visualization to understand data flow\n",
    "\n",
    "### 🔧 Exercises to Try:\n",
    "\n",
    "1. **Modify the attention mechanism** to use different attention patterns\n",
    "2. **Implement different positional encodings** (learnable vs fixed)\n",
    "3. **Add more sophisticated masking** for different sequence tasks\n",
    "4. **Experiment with model sizes** and see how performance changes\n",
    "5. **Implement gradient clipping** and learning rate scheduling\n",
    "\n",
    "Remember: Building AI from scratch is challenging but incredibly rewarding! Each component you implement deepens your understanding of how modern AI systems work.\n",
    "\n",
    "Good luck with your AI assistant journey! 🤖✨"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
